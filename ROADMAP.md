# Fluid OS - Project Roadmap

## Vision
A bare-metal unikernel capable of running TinyLlama with real-time JIT optimization and persistent optimization cache across reboots, targeting dedicated inference machines where the kernel and modules are compiled on-host (clang/LLVM + llvm-libc) to squeeze every CPU feature available.

## Current Status
‚úÖ Two-stage bootloader (Stage 1 + Stage 2)
‚úÖ 32-bit protected mode kernel
‚úÖ VGA text mode driver
‚úÖ PS/2 keyboard input
‚úÖ AOT module system with cycle-accurate profiling
‚úÖ Basic memory management (malloc/free)
‚úÖ LLVM 18 JIT interface (userspace prototype)

## Phase 1: JIT Integration & Module System (IN PROGRESS)

### 1.1 Runtime Infrastructure (COMPLETED ‚úÖ)
- [x] Implement minimal C++ runtime for bare-metal
  - [x] Operator new/delete with custom allocator
  - [x] Basic exception handling stubs (or -fno-exceptions)
  - [x] RTTI stubs (or -fno-rtti)
  - [x] Static constructor/destructor support
- [x] Custom memory allocator (jit_allocator)
  - [x] Three memory pools (CODE, DATA, METADATA)
  - [x] Aligned allocation support
  - [x] Free-list with block coalescence
  - [x] Pool statistics tracking
- [x] Bootloader support for 80-sector kernel (40KB)
  - [x] Chunked LBA reading (8 sectors at a time, 10 iterations)
  - [x] Kernel loaded at 0x10000 to avoid bootloader conflict
  - [x] Segment:offset addressing for real mode

### 1.2 Profile-Guided Optimization System ‚úÖ COMPLETED & VALIDATED

**Architecture Decision**: AOT + Offline Recompilation (not bare-metal JIT)
- Kernel profiles hot functions with cycle-accurate rdtsc
- Profiling data exported via serial port to host
- Host uses full LLVM toolchain to recompile with PGO
- Optimized modules loaded from persistent cache at boot
- Rationale: Full LLVM in bare-metal requires ~500KB + libc++ (3-6 months work)

**Status**: üéâ **100% FUNCTIONAL & VALIDATED** - Real performance gains measured!

**Completed Tasks**:
- [x] Profiling Export System
  - [x] JSON format for profiling statistics
  - [x] Serial port driver (COM1) for data extraction
  - [x] Export per-module call counts, cycles (min/max/total), code addresses
  - [x] Automated export at boot (no VGA interference)
- [x] Optimized Module Cache
  - [x] Design in-image cache format (embedded module blobs + registry)
  - [x] Cache loader at boot with header + module signature checks
  - [x] Cache tag generation (i686 for QEMU, native CPU for bare-metal)
  - [x] Module override system with fallback to embedded modules
- [x] Offline Recompilation Pipeline
  - [x] Implement `tools/pgo_recompile.py` to parse profiling JSON and emit plan
  - [x] Module classification: O1 (warm), O2 (hot), O3 (ultra-hot)
  - [x] Trigger LLVM recompilation with -O1/-O2/-O3 based on hotness
  - [x] Generate optimized .mod files in cache/i686/ directory
  - [x] Module linker script (modules/module.ld) ensuring predictable binary layout
  - [x] Entry_point fix: guarantee header at 0x00, code at 0x30
- [x] Benchmark Modules
  - [x] Add `matrix_mul` module (16√ó16 with .data section)
  - [x] Validated with compute, primes, sum, fibonacci modules
  - [x] Fixed .bss section issue - modules must use initialized data
- [x] Complete Workflow Testing
  - [x] make pgo-profile - Capture profiling from QEMU
  - [x] make pgo-analyze - Classify modules
  - [x] make pgo-apply - Recompile with optimizations
  - [x] Kernel rebuild with cache embedded
  - [x] Boot test: SUCCESS, no crashes ‚úÖ

**Key Files Created**:
- `modules/module.ld` - Linker script for predictable module layout
- `tools/pgo_recompile.py` - PGO analysis and recompilation
- `tools/gen_cpu_profile.py` - CPU profiling and tag generation
- `tools/embed_module.py` - Module embedding tool
- `tools/gen_cache_registry.py` - Cache registry generator
- `kernel/cache_loader.{h,c}` - Runtime cache loader

**Performance Measurements** (‚úÖ COMPLETED 2025-10-25):
- [x] Baseline measurements captured (compute: 6.6M cycles, fibonacci: 140K cycles)
- [x] Optimized measurements validated (compute: 3.6M cycles, fibonacci: 17K cycles)
- [x] Real gains measured:
  * fibonacci: **+87.59%** (nearly 8√ó faster with -O2)
  * compute: **+45.25%** (2√ó faster with -O3)
  * sum: **+47.00%**
  * primes: **+44.79%**
- [x] Critical bug fixed: -o3 vs -O3 flag generation in pgo_recompile.py
- [x] Full workflow validated: profile ‚Üí classify ‚Üí recompile ‚Üí embed ‚Üí boot ‚Üí measure

**Session 5 Progress** (2025-10-25):
- [x] Fixed 7-module loading bug (fft_1d, sha256, matrix_mul now load correctly)
- [x] Added stub functions to embedded_modules.h for cache override system
- [x] Validated complete PGO workflow with 7 modules (20 total calls)
- [x] Measured optimized performance: fibonacci 1.30x, compute 1.07x, fft_1d 1.09x, sha256 1.06x
- [x] Last working commit: caaf48d (7 modules, 50KB kernel)

**Session 6 Progress** (2025-10-25):
- [x] **RESOLVED 9-module boot failure** (bootloader sector limit)
- [x] Root cause: Bootloader reading 80 sectors (40KB), kernel was 50.7KB
- [x] Solution: Increased KERNEL_SECTORS from 80 to 128 (64KB capacity)
- [x] All 9 modules now boot successfully: fibonacci, sum, compute, primes, fft_1d, sha256, matrix_mul, quicksort, strops
- [x] Reduced heap size from 16MB to 256KB (reasonable for bare-metal)
- [x] Validated: "num_modules": 9, "total_calls": 22

**Remaining Optional Tasks** (not blocking):
- [ ] Disk partition/file for persistent storage (currently using embedded cache)
- [ ] LRU eviction policy for cache management
- [x] Fix matrix_mul static data issue (.bss ‚Üí .data section with initialized arrays)
- [x] Debug matrix_mul module loading/execution (FIXED - required stub in embedded_modules.h)
- [x] Debug 9-module boot failure (‚úÖ FIXED - increased bootloader capacity)

### 1.3 llvm-libc Integration & Toolchain ‚úÖ COMPLETED
- [x] Integrate llvm-libc subset (freestanding mode)
  - [x] Replace stdlib.c string functions (memcpy, memset, strlen, strcmp)
  - [x] Build system (Makefile.llvmlibc)
  - [x] Link kernel with libllvmlibc.a
  - [x] Add math.h functions (sin, cos, exp, log with Taylor series)
  - [ ] Adapt malloc/free to use jit_allocator backend (Phase 1.4)
- [ ] CPU Feature Detection Pipeline
  - [x] Host scanner: clang -march=native -### to detect features
  - [x] Generate build/cpu_profile.json (SSE/AVX/BMI + cache sizes)
  - [x] Auto-generate kernel/auto_cpu_flags.mk for Makefile
  - [ ] Runtime CPUID validation at boot
  - [ ] Tag benchmark results with CPU profile (e.g., matrix_mul_AVX2)
- [ ] Host-tuned build workflow
  - [x] Script tools/gen_cpu_profile.py for feature detection (default `-march=i686` safe for QEMU)
  - [ ] Compile kernel/modules with -march=native on target machine
  - [ ] Validate llvm-libc API coverage for benchmarks
  - [ ] Document llvm-libc gaps and workarounds

### 1.4 Module System Improvements (IN PROGRESS)
- [ ] Dynamic module loading from disk
  - [ ] FAT16 filesystem support (read-only, minimal)
  - [ ] Module loader from disk sectors
  - [ ] Module signature verification (SHA256)
- [ ] Enhanced profiling metrics
  - [ ] Per-function profiling granularity
  - [ ] Call graph tracking
  - [ ] Memory allocation per module
- [x] Additional benchmark modules (‚úÖ COMPLETED)
  - [x] `fft_1d`: radix-2 FFT (32 samples) - memory stride, trig, complex arithmetic
  - [x] `sha256`: SHA256 hash (1KB chunks) - memory bandwidth, bitwise ops
  - [x] `quicksort`: recursive sorting (128 elements, 5 iterations) - branch prediction tests
  - [x] `strops`: string operations (100 iterations) - memory access patterns
  - [x] Capture cycle counts via existing PGO workflow
- [x] Module loading system fixes (‚úÖ COMPLETED)
  - [x] Fixed 7-module loading (fft_1d, sha256, matrix_mul)
  - [x] Added stub functions for cache override system
  - [x] Validated module execution with profiling export
- [x] Debug boot failure with 9 modules (‚úÖ RESOLVED)
  - [x] Created quicksort.c and strops.c modules
  - [x] Root cause identified: Bootloader capacity (80 sectors/40KB) exceeded
  - [x] Solution: Increased bootloader to 128 sectors (64KB)
  - [x] All 9 modules now boot successfully (commit 4b5ce5c)
  - [x] Validation: "num_modules": 9, all tests passing

## Phase 2: Kernel Extensions

### 2.1 Disk I/O & Filesystem ‚úÖ ~95% COMPLETE
- [x] Serial port (COM1) ‚úÖ
- [x] Interrupt handling (IDT, PIC, Timer, Keyboard) ‚úÖ
- [x] FAT16 read-only filesystem ‚úÖ
- [x] ATA/IDE disk driver ‚úÖ
- [x] Module loading from disk ‚úÖ
- [x] Disk module loader (disk_module_loader.c) ‚úÖ
- [ ] PGO cache persistence to disk (tool created, integration pending)

### 2.2 Multicore Bootstrap ‚è≥ (Replaces "Basic Scheduler")
**Note**: NO multitasking! Unikernel with single application. Multicore for data parallelism only.

- [ ] AP (Application Processor) startup (APIC/SIPI)
- [ ] APIC (Advanced Programmable Interrupt Controller) setup
- [ ] Per-core stacks and data structures
- [ ] Work distribution API (not task scheduler!)
- [ ] Core affinity for module execution
- [ ] Parallel tensor operations (for TinyLlama)

### 2.3 Additional Drivers (Optional)
- [x] Serial port (COM1) for debugging ‚úÖ
- [ ] PCI enumeration
- [ ] Network card (for remote profiling/telemetry)
- [ ] AHCI (if IDE insufficient)

## Phase 3: Runtime JIT Optimization üî• CRITICAL PATH

**Goal**: On-the-fly recompilation, NOT offline PGO!

### 3.1 Bitcode Module System (Week 1-2)
- [ ] Design bitcode module format (.bc instead of .mod)
  - [ ] LLVM bitcode wrapper with header
  - [ ] Entry point metadata
  - [ ] Module versioning
- [ ] Bitcode loader (load .bc from disk)
- [ ] Test with simple modules

### 3.2 Minimal JIT Integration (Week 3-4)
- [ ] LLVM C API integration (LLVMExecutionEngine)
- [ ] Static LLVM linking (~500KB overhead)
- [ ] Thread-local storage (TLS) for C++ runtime
- [ ] JIT compile at O0 (fast compilation)
- [ ] Execute and profile

### 3.3 Hot-Path Recompilation (Week 5-6)
- [ ] Recompile trigger based on call count
  - [ ] 100 calls ‚Üí O1
  - [ ] 1000 calls ‚Üí O2
  - [ ] 10000 calls ‚Üí O3
- [ ] Atomic code pointer swap
- [ ] Background recompilation (non-blocking)
- [ ] Code cache management

### 3.4 Alternative: Custom Micro-JIT (if LLVM too heavy)
- [ ] x86 code generator for hot loops
- [ ] Loop unrolling emitter
- [ ] SIMD code generation
- [ ] ~10KB footprint (vs 500KB LLVM)

### 3.5 Advanced Profiling (Hardware Counters)
- [ ] Hardware performance counters (PMU)
  - [ ] Branch prediction statistics
  - [ ] Cache miss rates
  - [ ] TLB misses
- [ ] Sampling-based profiling
- [ ] Flame graph generation (export data)
- [ ] Real-time optimization triggers
- [ ] Benchmark (tiled compute): `gemm_tile` module to validate cache hits and allocator behaviour
- [ ] Benchmark (physics loop): `physics_step` module to measure SIMD/vector gains
- [ ] Auto-optimization maturity
  - [ ] Graduated thresholds per module category
  - [ ] Tune reoptimization policies for TinyLlama preparation
- [ ] Hardware-awareness
  - [ ] Leverage PMU counters for SSE/AVX utilization feedback
  - [ ] Plan NUMA-aware memory placement for multi-socket deployments
  - [ ] Track llvm-libc feature coverage for bare-metal runtime
  - [ ] Inventory critical llvm-libc APIs (benchmarks + TinyLlama) and verify availability
  - [ ] Correlate bench tags/reporting with PMU metrics (e.g., `gemm_tile_AVX512`)
  - [ ] Plan llvm-libc massage/patch strategy (enumerate missing pieces, fallback APIs)

### 3.3 Memory Management
- [ ] Proper heap allocator (buddy system or slab)
- [ ] Page allocator
- [ ] Memory pools for JIT code
- [ ] Garbage collection for dead JIT code

## Phase 4: Infrastructure

### 4.1 Build System
- [ ] Unified Makefile for all components
- [ ] Automatic dependency tracking
- [ ] Cross-compilation toolchain setup script
- [ ] CI/CD pipeline (GitHub Actions)

### 4.2 Testing
- [ ] Unit tests for kernel functions
- [ ] Integration tests for module system
- [ ] JIT correctness tests
- [ ] Performance regression tests
- [ ] Automated QEMU-based tests

### 4.3 Documentation
- [ ] Architecture documentation
- [ ] API reference
- [ ] Developer guide
- [ ] Performance tuning guide
- [ ] Troubleshooting guide

## Phase 5: TinyLlama Integration (FINAL GOAL)

### 5.1 TinyLlama Port
- [ ] Analyze TinyLlama dependencies
  - [ ] Identify required math functions
  - [ ] Identify memory requirements
  - [ ] Identify system calls needed
- [ ] Port to bare-metal
  - [ ] Implement required libc functions
  - [ ] Implement math library (libm)
  - [ ] Implement memory management for tensors
- [ ] LLVM IR generation for TinyLlama
  - [ ] Compile TinyLlama to LLVM bitcode
  - [ ] Link with kernel runtime
  - [ ] Create loadable module

### 5.2 Real-Time Optimization
- [ ] Profile TinyLlama execution
  - [ ] Identify hot loops
  - [ ] Identify memory bottlenecks
  - [ ] Identify branch patterns
- [ ] Dynamic recompilation
  - [ ] JIT compile hot functions at O0 (fast compile)
  - [ ] Recompile at O1 after 100 calls
  - [ ] Recompile at O2 after 1000 calls
  - [ ] Recompile at O3 for ultra-hot paths
- [ ] Specialized optimizations
  - [ ] SIMD vectorization (SSE/AVX if available)
  - [ ] Loop unrolling
  - [ ] Constant propagation for weights
  - [ ] Inlining for small functions
- [ ] Host-tuned deployment
  - [ ] Compile kernel/modules on target machine with llvm-libc toolchain
  - [ ] Validate runtime CPU feature alignment with build flags via pipeline reports
  - [ ] Record inference metrics pre/post host-specific tuning
  - [ ] Capture bench metadata using generated profile tags (e.g., `matrix_mul_AVX2`)
  - [ ] Massage/patch llvm-libc as needed for TinyLlama runtime gaps (document diffs)

## CPU Feature Detection Pipeline
1. **Host Scanner (build time)**  
   Run `clang -march=native -###` or `llvm-mca` helper to emit supported extensions (SSE, AVX, BMI, etc.) and capture `/proc/cpuinfo` for reference.
2. **Profile Export**  
   Normalize features into `build/cpu_profile.json` (flags list + cache sizes + core count). Commit or distribute with the build artifacts.
3. **Flag Injection**  
   Generate `kernel/auto_cpu_flags.mk` consumed by Makefiles to set `-mattr=` / `-march=` consistently for kernel, modules, and llvm-libc.
4. **Runtime Sanity Check**  
   Early boot routine reads CPUID, re-validates expected features, and logs discrepancies. If mismatch, fall back to conservative path or halt.
5. **Reporting**  
   Bench harness consumes the JSON to tag results (e.g., `matrix_mul_AVX2`) so comparisons across machines remain coherent.

## Benchmark Catalog
- **matrix_mul**  
  Dense 64√ó64 / 128√ó128 multiplication. Validates JIT threshold switching and SIMD gains; report cycles per multiply and reoptimization deltas.
- **fft_1d**  
  In-place radix-2 FFT on 1‚ÄØk samples. Exercises trig tables and strided memory; track cycles per transform and cache effects post-optimization.
- **sha256_stream**  
  Hash 1‚ÄØMB buffer chunks. Highlights memory bandwidth and pipeline efficiency; report throughput (MB/s) and instructions per block.
- **regex_dfa**  
  DFA-based pattern matcher over synthetic logs. Stresses branch prediction; compare misprediction counts and average cycles per character.
- **gemm_tile**  
  Tiled GEMM with configurable tile size. Validates allocator + cache behavior; collect cache hit rates and optimal tile selection flow.
- **physics_step**  
  Particle integrator (Verlet + collision). Mixes math/branching; track SIMD utilization and per-step latency, useful precursor to TinyLlama loops.***

### 5.3 Persistent Optimization Cache
- [ ] Boot-time cache loading
  - [ ] Load cached optimized functions
  - [ ] Verify cache integrity
  - [ ] Apply relocations
- [ ] Runtime cache updates
  - [ ] Write new optimized code to cache
  - [ ] Update cache metadata
  - [ ] Sync cache to disk periodically
- [ ] Cache management
  - [ ] Monitor cache size
  - [ ] Evict cold code
  - [ ] Defragment cache
  - [ ] Version migration

### 5.4 TinyLlama Demo
- [ ] Interactive prompt in kernel
- [ ] Load TinyLlama model from disk
- [ ] Run inference with JIT optimization
- [ ] Display profiling statistics
- [ ] Demonstrate cache persistence across reboots

## Success Metrics

### Performance Goals
- [ ] TinyLlama inference: < 1 second latency for short prompts
- [ ] JIT compilation overhead: < 100ms for hot functions
- [ ] Cache hit rate: > 90% after warmup
- [ ] Boot time: < 5 seconds to interactive prompt

### Code Quality
- [ ] Test coverage: > 80%
- [ ] Documentation coverage: 100% of public APIs
- [ ] Zero compiler warnings
- [ ] Clean QEMU execution (no CPU exceptions)

### Innovation
- [ ] First bare-metal LLM with JIT optimization
- [ ] Persistent JIT cache across reboots
- [ ] Real-time adaptive optimization
- [ ] Minimal memory footprint (< 512MB for model + runtime)

## Timeline Estimate

- **Phase 1**: 4-6 weeks (JIT + Modules)
- **Phase 2**: 3-4 weeks (Kernel extensions)
- **Phase 3**: 4-6 weeks (Optimizations)
- **Phase 4**: 2-3 weeks (Infrastructure)
- **Phase 5**: 8-12 weeks (TinyLlama integration)

**Total**: 21-31 weeks (~5-8 months)

## Notes

- Focus on incremental progress: each phase should produce a working system
- Prioritize JIT cache persistence early (Phase 3.1) as it's core to the project
- Consider memory constraints: TinyLlama requires ~1GB for model + activations
- May need to upgrade to 64-bit kernel for larger address space
- SIMD optimizations (SSE/AVX) will be critical for performance

---

## Session 8 Progress (2025-10-25 Afternoon)

### ‚úÖ Completed
- **Architecture Documentation**:
  - ARCHITECTURE_DECISIONS.md: Core principles (no scheduler, runtime JIT, multicore)
  - RUNTIME_JIT_PLAN.md: Full 10-week JIT integration plan
- **3 New Benchmark Modules**:
  - regex_dfa: DFA pattern matching (branch prediction)
  - gemm_tile: 32x32 tiled matrix multiply (cache behavior)
  - physics_step: 64-particle Verlet integration (mixed compute)
- **12-Module System**: Kernel now 82KB with 12 modules
- **Disk Module Loader**: Load .MOD files from FAT16
- **PGO Cache Sync Tool**: pgo_cache_sync.py for disk persistence

### üìã Next Steps
1. **Runtime JIT** (Phase 3.1-3.3): Bitcode modules + LLVM integration
2. **Multicore** (Phase 2.2): AP startup, work distribution
3. **TinyLlama** (Phase 5): Layer-wise JIT compilation

---

**Last Updated**: 2025-10-25 (Session 8)
**Status**: Phase 2.1 ‚úÖ **95% COMPLETE** - Disk I/O functional, 12 modules operational. Ready for Phase 3 (Runtime JIT) - CRITICAL PATH!
